# version: '3.8'

# services:
#   postgres_dw:
#     image: postgres:15
#     container_name: postgres_wheather
#     restart: unless-stopped
#     environment:
#       POSTGRES_USER: wheather_user
#       POSTGRES_PASSWORD: wheather_password
#       POSTGRES_DB: wheather_analytics
#     volumes:
#       - postgres_data:/var/lib/postgresql/data
#       - ./postgres_init:/docker-entrypoint-initdb.d
#     ports:
#       - "5433:5432"
#     networks:
#       - wheather_network

#   superset:
#     image: apache/superset:latest
#     container_name: wheather_superset
#     depends_on:
#       - postgres_dw
#     ports:
#       - "8088:8088"
#     environment:
#       SUPERSET_SECRET_KEY: "my_very_strong_secret_key_for_superset"
#     command: >
#       /bin/bash -c "
#       pip install psycopg2-binary &&
#       superset db upgrade &&
#       superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin &&
#       superset init &&
#       superset run -h 0.0.0.0 -p 8088"
#     volumes:
#       - superset_data:/app/superset_home
#     networks:
#       - wheather_network
 
#   spark-master:
#     image: bitnami/spark:3.5
#     container_name: wheather-spark-master
#     environment:
#       - SPARK_MODE=master
#       - SPARK_RPC_AUTHENTICATION_ENABLED=no
#       - SPARK_RPC_ENCRYPTION_ENABLED=no
#       - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
#       - SPARK_SSL_ENABLED=no
#     ports:
#       - '8080:8080'
#       - '7077:7077'
#     volumes:
#       - ./pyspark_apps:/opt/bitnami/spark/apps
#       - ./data_lake:/opt/data_lake
#     networks:
#       - wheather_network
 
#   spark-worker:
#     image: bitnami/spark:3.5
#     container_name: wheather-spark-worker
#     depends_on:
#       - spark-master
#     environment:
#       - SPARK_MODE=worker
#       - SPARK_MASTER_URL=spark://spark-master:7077
#       - SPARK_WORKER_MEMORY=1G
#       - SPARK_WORKER_CORES=1
#       - SPARK_RPC_AUTHENTICATION_ENABLED=no
#       - SPARK_RPC_ENCRYPTION_ENABLED=no
#       - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
#       - SPARK_SSL_ENABLED=no
#     volumes:
#       - ./pyspark_apps:/opt/pyspark_apps
#       - ./data_lake:/opt/data_lake
#     networks:
#       - wheather_network
 
#   airflow-init:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     image: airflow-custom:2.8.0
#     container_name: wheather-airflow-init
#     depends_on:
#       - postgres_dw
#     environment:
#       - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://dw_user:dw_password@postgres_dw:5432/ecommerce_warehouse
#       - _AIRFLOW_WWW_USER_USERNAME=admin
#       - _AIRFLOW_WWW_USER_PASSWORD=admin
#     volumes:
#       - ./airflow_dags:/opt/airflow/dags
#       - ./airflow_logs:/opt/airflow/logs
#       - ./airflow_plugins:/opt/airflow/plugins
#       - ./pyspark_apps/jars:/opt/jars
#       - ./data_lake:/opt/data_lake
#     command: >
#       bash -c "
#         airflow db init &&
#         airflow users create --username admin --password admin --firstname Air --lastname Flow --role Admin --email admin@example.com
#       "
#     networks:
#       - wheather_network
 
#   airflow-webserver:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     image: apache/airflow:2.8.0
#     restart: unless-stopped
#     depends_on:
#       - airflow-init
#       - postgres_dw
#       - spark-master
#     environment:
#       - AIRFLOW__CORE__EXECUTOR=LocalExecutor
#       - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://dw_user:dw_password@postgres_dw:5432/ecommerce_warehouse
#       - AIRFLOW__CORE__LOAD_EXAMPLES=False
#     volumes:
#       - ./airflow_dags:/opt/airflow/dags
#       - ./airflow_logs:/opt/airflow/logs
#       - ./airflow_plugins:/opt/airflow/plugins
#       - ./pyspark_apps:/opt/pyspark_apps
#       - ./data_lake:/opt/data_lake
#       - ./pyspark_apps/jars:/opt/jars
#     ports:
#       - "8081:8080"
#     command: webserver
#     networks:
#       - wheather_network
 
#   airflow-scheduler:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     image: apache/airflow:2.8.0
#     restart: unless-stopped
#     depends_on:
#       - airflow-init
#       - postgres_dw
#       - spark-master
#     environment:
#       - AIRFLOW__CORE__EXECUTOR=LocalExecutor
#       - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://dw_user:dw_password@postgres_dw:5432/ecommerce_warehouse
#       - AIRFLOW__CORE__LOAD_EXAMPLES=False
#     volumes:
#       - ./airflow_dags:/opt/airflow/dags
#       - ./airflow_logs:/opt/airflow/logs
#       - ./airflow_plugins:/opt/airflow/plugins
#       - ./pyspark_apps:/opt/pyspark_apps
#       - ./data_lake:/opt/data_lake
#       - ./pyspark_apps/jars:/opt/jars
#     command: scheduler
#     networks:
#       - wheather_network

# volumes:
#   postgres_data:
#   superset_data:
 
# networks:
#   wheather_network:
#     driver: bridge


# version: '3.8'

services:
  postgres_weather:
    image: postgres:15
    container_name: postgres_weather
    restart: unless-stopped
    environment:
      POSTGRES_USER: weather_user
      POSTGRES_PASSWORD: weather_password
      POSTGRES_DB: weather_analytics
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres_init:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432"
    networks:
      - weather_network

  weather_superset:
    image: apache/superset:latest
    container_name: weather_superset
    depends_on:
      - postgres_weather
    ports:
      - "8088:8088"
    environment:
      SUPERSET_SECRET_KEY: "my_very_strong_secret_key_for_superset"
    command: >
      /bin/bash -c "
      pip install psycopg2-binary &&
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088"
    volumes:
      - superset_data:/app/superset_home
    networks:
      - weather_network

  weather-spark-master:
    image: bitnami/spark:3.5
    container_name: weather-spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080'
      - '7077:7077'
    volumes:
      - ./pyspark_apps:/opt/pyspark_apps
      - ./data_lake:/opt/data_lake
    networks:
      - weather_network

  weather-spark-worker:
    image: bitnami/spark:3.5
    container_name: weather-spark-worker
    depends_on:
      - weather-spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://weather-spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./pyspark_apps:/opt/pyspark_apps
      - ./data_lake:/opt/data_lake
    networks:
      - weather_network

  weather-airflow-init:
    image: apache/airflow:2.8.0
    container_name: weather-airflow-init
    depends_on:
      - postgres_weather
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://weather_user:weather_password@postgres_weather:5432/weather_analytics
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
      - ./pyspark_apps:/opt/pyspark_apps
      - ./pyspark_apps/jars:/opt/jars
      - ./data_lake:/opt/data_lake
    command: >
      bash -c "
        pip install requests pandas pyspark pyarrow fastparquet sqlalchemy psycopg2-binary python-dotenv &&
        airflow db init &&
        airflow users create --username admin --password admin --firstname Air --lastname Flow --role Admin --email admin@example.com
      "
    networks:
      - weather_network

  airflow-webserver:
    image: apache/airflow:2.8.0
    restart: unless-stopped
    depends_on:
      - weather-airflow-init
      - postgres_weather
      - weather-spark-master
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://weather_user:weather_password@postgres_weather:5432/weather_analytics
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
      - ./pyspark_apps:/opt/pyspark_apps
      - ./pyspark_apps/jars:/opt/jars
      - ./data_lake:/opt/data_lake
    ports:
      - "8081:8080"
    command: >
      bash -c "
        pip install requests pandas pyspark pyarrow fastparquet sqlalchemy psycopg2-binary python-dotenv &&
        airflow webserver
      "
    networks:
      - weather_network

  airflow-scheduler:
    image: apache/airflow:2.8.0
    restart: unless-stopped
    depends_on:
      - weather-airflow-init
      - postgres_weather
      - weather-spark-master
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://weather_user:weather_password@postgres_weather:5432/weather_analytics
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./airflow_logs:/opt/airflow/logs
      - ./airflow_plugins:/opt/airflow/plugins
      - ./pyspark_apps:/opt/pyspark_apps
      - ./pyspark_apps/jars:/opt/jars
      - ./data_lake:/opt/data_lake
    command: >
      bash -c "
        pip install requests pandas pyspark pyarrow fastparquet sqlalchemy psycopg2-binary python-dotenv &&
        airflow scheduler
      "
    networks:
      - weather_network

volumes:
  postgres_data:
  superset_data:

networks:
  weather_network:
    driver: bridge

